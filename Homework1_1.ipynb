{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.datasets import load_digits\n", "from sklearn.preprocessing import OneHotEncoder\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["oading the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["digits = load_digits()\n", "data = digits.data\n", "target = digits.target\n", "minibatch_size = 20"]}, {"cell_type": "markdown", "metadata": {}, "source": ["esaping the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = data / 16.0  # Rescale to [0, 1]\n", "data = data.reshape(data.shape[0], -1)  # Reshape to (64,) vectors"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ncoding data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["encoder = OneHotEncoder(sparse_output=False, categories='auto')\n", "target_encoded = encoder.fit_transform(target.reshape(-1, 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(data[0].shape)\n", "print(target_encoded[0].shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["unction used to visualize one digit"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def data_visual(data, target, i):\n", "    plt.figure(figsize=(8, 8))\n", "    plt.imshow(data[i].reshape(8, 8), cmap='gray')\n", "    plt.title(target[i])\n", "    plt.axis('off')\n", "    plt.show() "]}, {"cell_type": "markdown", "metadata": {}, "source": ["unction used to creat a mini batch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def data_generator(input_data, target_data, minibatch_size):\n", "    # Get the total number of samples in the dataset\n", "    num_samples = len(input_data)\n", "    \n", "    # Create an array of indices corresponding to the samples\n", "    indices = np.arange(num_samples)\n", "    \n", "    # Shuffle the indices to randomize the order of samples\n", "    np.random.shuffle(indices)\n\n", "    # Iterate over the shuffled indices to yield mini-batches\n", "    for start_idx in range(0, num_samples - minibatch_size + 1, minibatch_size):\n", "        # Select a subset of indices for the current mini-batch\n", "        excerpt = indices[start_idx:start_idx + minibatch_size]\n", "        \n", "        # Yield the corresponding input and target data for the mini-batch\n", "        yield input_data[excerpt], target_data[excerpt]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SigmoidActivation:\n", "    def __call__(self, inputs):\n", "        \"\"\"\n", "        Apply the sigmoid activation function to the input.\n", "        Args:\n", "        inputs (ndarray): Input data of shape (minibatch_size, num_units).\n", "        Returns:\n", "        ndarray: Output data after applying the sigmoid activation function.\n", "        \"\"\"\n", "        return 1 / (1 + np.exp(-inputs))\n", "    def backward(self, pre_activation, activation, error_signal):\n", "        \"\"\"\n", "        Calculate the gradient of the loss with respect to the pre-activation (dL/dpre-activation).\n", "        Args:\n", "        pre_activation (ndarray): Pre-activation values of shape (minibatch_size, num_units).\n", "        activation (ndarray): Activation values (output of the activation function) of the same shape.\n", "        error_signal (ndarray): Gradient of the loss with respect to the activation (dL/dactivation).\n", "        Returns:\n", "        ndarray: Gradient of the loss with respect to the pre-activation (dL/dpre-activation).\n", "        \"\"\"\n", "        # Calculate the gradient of the sigmoid activation function\n", "        dactivation_dpre_activation = activation * (1 - activation)\n", "        \n", "        # Multiply the gradient of the activation with the error signal to get the gradient of the loss\n", "        dL_dpre_activation = dactivation_dpre_activation * error_signal\n", "        \n", "        return dL_dpre_activation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SoftmaxActivation:\n", "    def __call__(self, inputs):\n", "        \"\"\"\n", "        Apply the softmax activation function to the input.\n", "        Args:\n", "        inputs (ndarray): Input data of shape (minibatch_size, 10), where each row represents a vector.\n", "        Returns:\n", "        ndarray: Output data after applying the softmax activation function, with the same shape as inputs.\n", "        \"\"\"\n", "        # Calculate the exponential of the inputs\n", "        exp_inputs = np.exp(inputs)\n", "        \n", "        # Sum the exponentials along the second axis to compute the denominator\n", "        denominator = np.sum(exp_inputs, axis=1, keepdims=True)\n", "        \n", "        # Compute the softmax probabilities by dividing the exponentials by the denominator\n", "        softmax_output = exp_inputs / denominator\n", "        \n", "        return softmax_output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MLPLayer:\n", "    def __init__(self, activation, num_units, input_size):\n", "        \"\"\"\n", "        Initialize the MLP layer.\n", "        Args:\n", "        activation: An instance of an activation function (SigmoidActivation or SoftmaxActivation).\n", "        num_units (int): Number of units (perceptrons) in the layer.\n", "        input_size (int): Number of units in the preceding layer.\n", "        \"\"\"\n", "        self.activation = activation\n", "        self.num_units = num_units\n", "        self.input_size = input_size\n", "        \n", "        # Initialize weights with small random values (e.g., normal distribution with \u00ce\u00bc=0, \u00cf\u0192=0.2)\n", "        self.weights = np.random.normal(0, 0.2, size=(input_size, num_units))\n", "        \n", "        # Initialize bias values to zero\n", "        self.bias = np.zeros(num_units)\n", "    \n", "    def forward(self, input_data):\n", "        \"\"\"\n", "        Apply the forward pass of the MLP layer.\n", "        Args:\n", "        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n", "        Returns:\n", "        ndarray: Output data after applying the weight matrix, bias, and activation function.\n", "        \"\"\"\n", "        # Calculate the pre-activations (z) by applying the weight matrix and adding the bias\n", "        z = np.dot(input_data, self.weights) + self.bias\n", "        \n", "        # Apply the activation function to the pre-activations\n", "        output = self.activation(z)\n", "        \n", "        return output\n", "    def backward_weights(self, dL_dpre_activation, pre_activation, input_data):\n", "        \"\"\"\n", "        Calculate the gradient of the loss with respect to the weights (dL/dW).\n", "        Args:\n", "        dL_dpre_activation (ndarray): Gradient of the loss with respect to pre-activation.\n", "        pre_activation (ndarray): Pre-activation values.\n", "        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n", "        Returns:\n", "        ndarray: Gradient of the loss with respect to the weights (dL/dW).\n", "        \"\"\"\n", "        minibatch_size = dL_dpre_activation.shape[0]\n", "        \n", "        # Calculate dL/dW using the outer product of the input and the gradient of the loss\n", "        dL_dW = np.dot(input_data.T, dL_dpre_activation) / minibatch_size\n", "        \n", "        return dL_dW\n", "    def backward_input(self, dL_dpre_activation, pre_activation, weights):\n", "        \"\"\"\n", "        Calculate the gradient of the loss with respect to the input (dL/dinput).\n", "        Args:\n", "        dL_dpre_activation (ndarray): Gradient of the loss with respect to pre-activation.\n", "        pre_activation (ndarray): Pre-activation values.\n", "        weights (ndarray): Weight matrix.\n", "        Returns:\n", "        ndarray: Gradient of the loss with respect to the input (dL/dinput).\n", "        \"\"\"\n", "        # Calculate dL/dinput using the dot product of the gradient of the loss and the weight matrix\n", "        dL_dinput = np.dot(dL_dpre_activation, weights.T)\n", "        \n", "        return dL_dinput\n", "    def backward(self, dL_dpre_activation, pre_activation, input_data):\n", "        \"\"\"\n", "        Calculate the gradients for the entire MLP layer.\n", "        Args:\n", "        dL_dpre_activation (ndarray): Gradient of the loss with respect to pre-activation.\n", "        pre_activation (ndarray): Pre-activation values.\n", "        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n", "        Returns:\n", "        dL_dW (ndarray): Gradient of the loss with respect to the weights (dL/dW).\n", "        dL_dinput (ndarray): Gradient of the loss with respect to the input (dL/dinput).\n", "        \"\"\"\n", "        dL_dactivation = self.activation.backward(pre_activation, dL_dpre_activation)\n", "        dL_dW = self.backward_weights(dL_dpre_activation, pre_activation, input_data)\n", "        dL_dinput = self.backward_input(dL_dpre_activation, pre_activation, self.weights)\n", "        \n", "        return dL_dW, dL_dinput"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MLP:\n", "    def __init__(self, layer_sizes, activation, input_size=64):\n", "        \"\"\"\n", "        Initialize the Multi-Layer Perceptron (MLP).\n", "        Args:\n", "        layer_sizes (list): List of integers specifying the number of units in each layer.\n", "        activation: An instance of an activation function (SigmoidActivation or SoftmaxActivation).\n", "        input_size (int): Number of input units (default is 64).\n", "        \"\"\"\n", "        self.layer_sizes = layer_sizes\n", "        self.activation = activation\n", "        \n", "        # Create a list to store all the MLP layers\n", "        self.layers = []\n", "        \n", "        # Initialize the input size for the first layer\n", "        layer_input_size = input_size\n", "        \n", "        # Create and append each layer to the list\n", "        for num_units in layer_sizes:\n", "            layer = MLPLayer(activation, num_units, layer_input_size)\n", "            self.layers.append(layer)\n", "            # Update the input size for the next layer\n", "            layer_input_size = num_units\n", "    \n", "    def forward(self, input_data):\n", "        \"\"\"\n", "        Apply the forward pass of the entire MLP.\n", "        Args:\n", "        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n", "        Returns:\n", "        ndarray: Output data after applying the entire MLP.\n", "        \"\"\"\n", "        # Initialize the input as the input data\n", "        output = input_data\n", "        \n", "        # Iterate through the layers and apply the forward pass\n", "        for layer in self.layers:\n", "            output = layer.forward(output)\n", "        \n", "        return output\n", "    def backward(self, predicted, target):\n", "        \"\"\"\n", "        Calculate the gradients for the entire MLP and update the weights.\n", "        Args:\n", "        predicted (ndarray): Predicted probabilities or logits (output of the model).\n", "        target (ndarray): True one-hot encoded target labels.\n", "        Returns:\n", "        None\n", "        \"\"\"\n", "        # Calculate the gradient of the CCE loss with respect to predicted\n", "        dL_dactivation = -target / predicted\n", "        \n", "        # Initialize lists of dictionaries to store activations, pre-activations, and weight gradients\n", "        activations_list = [{} for _ in range(len(self.layers))]\n", "        pre_activations_list = [{} for _ in range(len(self.layers))]\n", "        weight_gradients_list = [{} for _ in range(len(self.layers))]\n", "        \n", "        # Forward pass to collect activations and pre-activations\n", "        input_data = predicted  # The first \"activation\" is the model's output\n", "        for i, layer in enumerate(self.layers):\n", "            pre_activation = np.dot(input_data, layer.weights) + layer.bias\n", "            activations_list[i] = input_data\n", "            pre_activations_list[i] = pre_activation\n", "            input_data = layer.activation(pre_activation)\n", "        \n", "        # Backward pass to compute gradients and update weights\n", "        error_signal = dL_dactivation  # Error signal for the last layer\n", "        for i in reversed(range(len(self.layers))):\n", "            layer = self.layers[i]\n", "            dL_dW, dL_dinput = layer.backward(error_signal, pre_activations_list[i], activations_list[i])\n", "            weight_gradients_list[i]['dL_dW'] = dL_dW\n", "            error_signal = dL_dinput  # Update error signal for the next layer\n", "        \n", "        # Update the weights of all layers\n", "        for i, layer in enumerate(self.layers):\n", "            layer.weights -= weight_gradients_list[i]['dL_dW']\n", "            layer.bias -= np.mean(error_signal, axis=0)  # Update bias\n", "        \n", "        return None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class CategoricalCrossEntropyLoss:\n", "    def __call__(self, predicted, target):\n", "        \"\"\"\n", "        Calculate the categorical cross-entropy loss.\n", "        Args:\n", "        predicted (ndarray): Predicted probabilities or logits (output of the model).\n", "        target (ndarray): True one-hot encoded target labels.\n", "        Returns:\n", "        float: The loss value.\n", "        \"\"\"\n", "        # Ensure numerical stability by adding a small epsilon to avoid log(0)\n", "        epsilon = 1e-15\n", "        #predicted = np.clip(predicted, epsilon, 1 - epsilon)\n\n", "        # Calculate the cross-entropy loss\n", "        loss = -np.sum(target * np.log(predicted)) / predicted.shape[0]\n", "        return loss\n", "    def backward(self, predicted, loss):\n", "        \"\"\"\n", "        Calculate the gradient of the loss with respect to the predicted probabilities.\n", "        Args:\n", "        predicted (ndarray): Predicted probabilities or logits (output of the model).\n", "        loss (ndarray): Loss value of shape (minibatch_size, 1).\n", "        Returns:\n", "        ndarray: Gradient of the loss with respect to the predicted probabilities of shape (minibatch_size, 10).\n", "        \"\"\"\n", "        minibatch_size = predicted.shape[0]\n", "        \n", "        # Initialize an array for the gradients\n", "        gradient = np.zeros_like(predicted)\n", "        \n", "        # Calculate the gradient for each sample in the minibatch\n", "        for i in range(minibatch_size):\n", "            # Compute the derivative of the loss with respect to each element of predicted\n", "            dLCCE_dy = -target[i] / predicted[i]\n", "            \n", "            # Apply the chain rule to get the gradient of the loss with respect to predicted\n", "            d\u00cb\u2020y_dz = predicted[i] * (1 - predicted[i])\n", "            \n", "            # Multiply the results to obtain the gradient\n", "            gradient[i] = dLCCE_dy * d\u00cb\u2020y_dz\n", "        \n", "        return gradient"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_mlp(mlp, data, target_encoded, learning_rate, num_epochs, minibatch_size):\n", "    loss_history = []\n", "    accuracy_history = []\n", "    \n", "    for epoch in range(1, num_epochs + 1):\n", "        # Shuffle the data and target_encoded\n", "        indices = np.arange(len(data))\n", "        np.random.shuffle(indices)\n", "        data_shuffled = data[indices]\n", "        target_shuffled = target_encoded[indices]\n", "        \n", "        total_loss = 0\n", "        correct_predictions = 0\n", "        \n", "        for start_idx in range(0, len(data), minibatch_size):\n", "            end_idx = start_idx + minibatch_size\n", "            inputs = data_shuffled[start_idx:end_idx]\n", "            targets = target_shuffled[start_idx:end_idx]\n", "            \n", "            # Forward pass\n", "            predictions = mlp.forward(inputs)\n", "            \n", "            print(predictions)\n\n", "            # Calculate loss\n", "            loss = cce_loss(predictions, targets)\n", "            total_loss += loss\n", "            \n", "            # Calculate accuracy\n", "            correct_predictions += np.sum(np.argmax(predictions, axis=1) == np.argmax(targets, axis=1))\n", "            \n", "            # Backward pass and weight updates\n", "            mlp.backward(predictions, targets)\n", "        \n", "        # Calculate average loss and accuracy for the epoch\n", "        avg_loss = total_loss / len(data)\n", "        accuracy = correct_predictions / len(data)\n", "        \n", "        loss_history.append(avg_loss)\n", "        accuracy_history.append(accuracy)\n", "        \n", "        print(f\"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy * 100:.2f}%\")\n", "    \n", "    # Plot the loss and accuracy\n", "    plt.figure(figsize=(12, 4))\n", "    plt.subplot(1, 2, 1)\n", "    plt.plot(range(1, num_epochs + 1), loss_history)\n", "    plt.title('Average Loss vs. Epoch')\n", "    plt.xlabel('Epoch')\n", "    plt.ylabel('Average Loss')\n", "    \n", "    plt.subplot(1, 2, 2)\n", "    plt.plot(range(1, num_epochs + 1), accuracy_history)\n", "    plt.title('Accuracy vs. Epoch')\n", "    plt.xlabel('Epoch')\n", "    plt.ylabel('Accuracy')\n", "    \n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Example usage:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_epochs = 10\n", "learning_rate = 0.01\n", "mlp = MLP(layer_sizes=[128, 64, 32], activation=SigmoidActivation(), input_size=64)\n", "cce_loss = CategoricalCrossEntropyLoss()\n", "train_mlp(mlp, data, target_encoded, learning_rate, num_epochs, minibatch_size)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}