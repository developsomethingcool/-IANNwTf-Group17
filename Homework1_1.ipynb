{"cells":[{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_digits\n","from sklearn.preprocessing import OneHotEncoder\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["Loading the data"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["digits = load_digits()\n","data = digits.data\n","target = digits.target\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Shaping the data"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["data = data / 16.0  # Rescale to [0, 1]"]},{"cell_type":"markdown","metadata":{},"source":["Encoding data"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["encoder = OneHotEncoder(sparse_output=False, categories='auto')\n","target_encoded = encoder.fit_transform(target.reshape(-1, 1))"]},{"cell_type":"markdown","metadata":{},"source":["Function used to visualize one digit"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["def data_visual(data, target, i):\n","    plt.figure(figsize=(8, 8))\n","    plt.imshow(data[i].reshape(8, 8), cmap='gray')\n","    plt.title(target[i])\n","    plt.axis('off')\n","    plt.show() "]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["data_visual(data, target, 678)"]},{"cell_type":"markdown","metadata":{},"source":["Function used to creat a mini batch"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["def data_generator(input_data, target_data, minibatch_size):\n","    # Get the total number of samples in the dataset\n","    num_samples = len(input_data)\n","    \n","    # Create an array of indices corresponding to the samples\n","    indices = np.arange(num_samples)\n","    # Shuffle the indices to randomize the order of samples\n","    np.random.shuffle(indices)\n","\n","    # Iterate over the shuffled indices to yield mini-batches\n","    for start_idx in range(0, num_samples - minibatch_size + 1, minibatch_size):\n","        # Select a subset of indices for the current mini-batch\n","        excerpt = indices[start_idx:start_idx + minibatch_size]\n","        \n","        # Yield the corresponding input and target data for the mini-batch\n","        yield input_data[excerpt], target_data[excerpt]"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["class SigmoidActivation:\n","    def __call__(self, inputs):\n","        \"\"\"\n","        Apply the sigmoid activation function to the input.\n","        Args:\n","        inputs (ndarray): Input data of shape (minibatch_size, num_units).\n","        Returns:\n","        ndarray: Output data after applying the sigmoid activation function.\n","        \"\"\"\n","        return 1 / (1 + np.exp(-inputs))\n","    def backward(self, pre_activation, activation, error_signal):\n","        \"\"\"\n","        Calculate the gradient of the loss with respect to the pre-activation (dL/dpre-activation).\n","        Args:\n","        pre_activation (ndarray): Pre-activation values of shape (minibatch_size, num_units).\n","        activation (ndarray): Activation values (output of the activation function) of the same shape.\n","        error_signal (ndarray): Gradient of the loss with respect to the activation (dL/dactivation).\n","        Returns:\n","        ndarray: Gradient of the loss with respect to the pre-activation (dL/dpre-activation).\n","        \"\"\"\n","        # Calculate the gradient of the sigmoid activation function\n","        dactivation_dpre_activation = activation * (1 - activation)\n","        \n","        # Multiply the gradient of the activation with the error signal to get the gradient of the loss\n","        dL_dpre_activation = dactivation_dpre_activation * error_signal\n","        \n","        return dL_dpre_activation"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["class SoftmaxActivation:\n","    def __call__(self, inputs):\n","        \"\"\"\n","        Apply the softmax activation function to the input.\n","        Args:\n","        inputs (ndarray): Input data of shape (minibatch_size, 10), where each row represents a vector.\n","        Returns:\n","        ndarray: Output data after applying the softmax activation function, with the same shape as inputs.\n","        \"\"\"\n","        # Calculate the exponential of the inputs\n","        exp_inputs = np.exp(inputs)\n","        \n","        # Sum the exponentials along the second axis to compute the denominator\n","        denominator = np.sum(exp_inputs, axis=1, keepdims=True)\n","        \n","        # Compute the softmax probabilities by dividing the exponentials by the denominator\n","        softmax_output = exp_inputs / denominator\n","        \n","        return softmax_output"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["class MLPLayer:\n","    def __init__(self, activation, num_units, input_size):\n","        \"\"\"\n","        Initialize the MLP layer.\n","        Args:\n","        activation: An instance of an activation function (SigmoidActivation or SoftmaxActivation).\n","        num_units (int): Number of units (perceptrons) in the layer.\n","        input_size (int): Number of units in the preceding layer.\n","        \"\"\"\n","        self.activation = activation\n","        self.num_units = num_units\n","        self.input_size = input_size\n","        \n","        # Initialize weights with small random values (e.g., normal distribution with Î¼=0, Ïƒ=0.2)\n","        self.weights = np.random.normal(0, 0.2, size=(input_size, num_units))\n","        \n","        # Initialize bias values to zero\n","        self.bias = np.zeros(num_units)\n","    \n","    def forward(self, input_data):\n","        \"\"\"\n","        Apply the forward pass of the MLP layer.\n","        Args:\n","        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n","        Returns:\n","        ndarray: Output data after applying the weight matrix, bias, and activation function.\n","        \"\"\"\n","        # Calculate the pre-activations (z) by applying the weight matrix and adding the bias\n","        z = np.dot(input_data, self.weights) + self.bias\n","        \n","        # Apply the activation function to the pre-activations\n","        output = self.activation(z)\n","        \n","        return output\n","    def backward_weights(self, dL_dpre_activation, pre_activation, input_data):\n","        \"\"\"\n","        Calculate the gradient of the loss with respect to the weights (dL/dW).\n","        Args:\n","        dL_dpre_activation (ndarray): Gradient of the loss with respect to pre-activation.\n","        pre_activation (ndarray): Pre-activation values.\n","        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n","        Returns:\n","        ndarray: Gradient of the loss with respect to the weights (dL/dW).\n","        \"\"\"\n","        minibatch_size = dL_dpre_activation.shape[0]\n","        \n","        # Calculate dL/dW using the outer product of the input and the gradient of the loss\n","        dL_dW = np.dot(input_data.T, dL_dpre_activation) / minibatch_size\n","        \n","        return dL_dW\n","    def backward_input(self, dL_dpre_activation, pre_activation, weights):\n","        \"\"\"\n","        Calculate the gradient of the loss with respect to the input (dL/dinput).\n","        Args:\n","        dL_dpre_activation (ndarray): Gradient of the loss with respect to pre-activation.\n","        pre_activation (ndarray): Pre-activation values.\n","        weights (ndarray): Weight matrix.\n","        Returns:\n","        ndarray: Gradient of the loss with respect to the input (dL/dinput).\n","        \"\"\"\n","        # Calculate dL/dinput using the dot product of the gradient of the loss and the weight matrix\n","        dL_dinput = np.dot(dL_dpre_activation, weights.T)\n","        \n","        return dL_dinput\n","    def backward(self, dL_dpre_activation, pre_activation, input_data):\n","        \"\"\"\n","        Calculate the gradients for the entire MLP layer.\n","        Args:\n","        dL_dpre_activation (ndarray): Gradient of the loss with respect to pre-activation.\n","        pre_activation (ndarray): Pre-activation values.\n","        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n","        Returns:\n","        dL_dW (ndarray): Gradient of the loss with respect to the weights (dL/dW).\n","        dL_dinput (ndarray): Gradient of the loss with respect to the input (dL/dinput).\n","        \"\"\"\n","        dL_dactivation = self.activation.backward(pre_activation, dL_dpre_activation)\n","        dL_dW = self.backward_weights(dL_dpre_activation, pre_activation, input_data)\n","        dL_dinput = self.backward_input(dL_dpre_activation, pre_activation, self.weights)\n","        \n","        return dL_dW, dL_dinput"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["class MLP:\n","    def __init__(self, layer_sizes, activation, input_size):\n","        # Initialize an empty list to store MLP layers\n","        self.layers = []\n","\n","        for i in range(len(layer_sizes)):\n","            if i == 0:\n","                # The first layer takes the input size\n","                self.layers.append(MLPLayer(activation, layer_sizes[i], input_size))\n","            else:\n","                # Subsequent layers take the number of units from the previous layer\n","                self.layers.append(MLPLayer(activation, layer_sizes[i], layer_sizes[i - 1]))\n","\n","        # Add the final layer with 10 output units and softmax activation\n","        self.layers.append(MLPLayer(SoftmaxActivation(), 10, layer_sizes[-1]))\n","\n","    \n","    def forward(self, input_data):\n","        \"\"\"\n","        Apply the forward pass of the entire MLP.\n","\n","        Args:\n","        input_data (ndarray): Input data of shape (minibatch_size, input_size).\n","\n","        Returns:\n","        ndarray: Output data after applying the entire MLP.\n","        \"\"\"\n","        # Initialize the input as the input data\n","        output = input_data\n","        \n","        # Iterate through the layers and apply the forward pass\n","        for layer in self.layers:\n","            output = layer.forward(output)\n","        \n","        return output\n","\n","\n","    def backward(self, predicted, target):\n","        \"\"\"\n","        Calculate the gradients for the entire MLP and update the weights.\n","\n","        Args:\n","        predicted (ndarray): Predicted probabilities or logits (output of the model).\n","        target (ndarray): True one-hot encoded target labels.\n","\n","        Returns:\n","        None\n","        \"\"\"\n","        # Calculate the gradient of the CCE loss with respect to predicted\n","        dL_dactivation = -target / predicted\n","        \n","        # Initialize lists of dictionaries to store activations, pre-activations, and weight gradients\n","        activations_list = [{} for _ in range(len(self.layers))]\n","        pre_activations_list = [{} for _ in range(len(self.layers))]\n","        weight_gradients_list = [{} for _ in range(len(self.layers))]\n","        \n","        # Initialize the input_data with the model's output (predicted)\n","        input_data = predicted  # The first \"activation\" is the model's output\n","\n","        # Forward pass to collect activations and pre-activations\n","        for i, layer in enumerate(self.layers):\n","            # Calculate the pre-activation\n","            pre_activation = np.dot(input_data, layer.weights) + layer.bias\n","\n","            # Store the activation and pre-activation in the respective lists\n","            activations_list[i]['activation'] = input_data\n","            pre_activations_list[i]['pre_activation'] = pre_activation\n","\n","            # Update input_data for the next layer\n","            input_data = layer.activation(pre_activation)\n","        \n","        # Backward pass to compute gradients and update weights\n","        error_signal = dL_dactivation  # Error signal for the last layer\n","        for i in reversed(range(len(self.layers))):\n","            layer = self.layers[i]\n","            dL_dW, dL_dinput = layer.backward(error_signal, pre_activations_list[i]['pre_activation'], activations_list[i]['activation'])\n","            weight_gradients_list[i]['dL_dW'] = dL_dW\n","            error_signal = dL_dinput  # Update error signal for the next layer\n","        \n","        # Update the weights of all layers and bias of the last layer\n","        for i, layer in enumerate(self.layers):\n","            layer.weights -= weight_gradients_list[i]['dL_dW']\n","            if i == len(self.layers) - 1:\n","                # Update bias only for the last layer\n","                layer.bias -= np.mean(error_signal, axis=0)\n","        \n","        return None\n"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["class CategoricalCrossEntropyLoss:\n","    def __call__(self, predicted, target):\n","        \"\"\"\n","        Calculate the categorical cross-entropy loss.\n","        Args:\n","        predicted (ndarray): Predicted probabilities or logits (output of the model).\n","        target (ndarray): True one-hot encoded target labels.\n","        Returns:\n","        float: The loss value.\n","        \"\"\"\n","        # Ensure numerical stability by adding a small epsilon to avoid log(0)\n","        epsilon = 1e-15\n","        #predicted = np.clip(predicted, epsilon, 1 - epsilon)\n","\n","        # Calculate the cross-entropy loss\n","\n","        loss = -np.sum(target * np.log(predicted)) / predicted.shape[0]\n","        return loss\n","    \n","    def backward(self, predicted, loss):\n","        \"\"\"\n","        Calculate the gradient of the loss with respect to the predicted probabilities.\n","\n","        Args:\n","        predicted (ndarray): Predicted probabilities or logits (output of the model).\n","        loss (ndarray): Loss value of shape (minibatch_size, 1).\n","\n","        Returns:\n","        ndarray: Gradient of the loss with respect to the predicted probabilities of shape (minibatch_size, 10).\n","        \"\"\"\n","        minibatch_size = predicted.shape[0]\n","        \n","        # Initialize an array for the gradients\n","        gradient = np.zeros_like(predicted)\n","        \n","        # Calculate the gradient for each sample in the minibatch\n","        for i in range(minibatch_size):\n","            # Compute the derivative of the loss with respect to each element of predicted\n","            dLCCE_dy = -target[i] / predicted[i]\n","            \n","            # Apply the chain rule to get the gradient of the loss with respect to predicted\n","            dˆy_dz = predicted[i] * (1 - predicted[i])\n","            \n","            # Multiply the results to obtain the gradient\n","            gradient[i] = dLCCE_dy * dˆy_dz\n","        \n","        return gradient"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["def train_mlp(mlp, data, target_encoded, learning_rate, num_epochs, minibatch_size):\n","    loss_history = []\n","    accuracy_history = []\n","    \n","    for epoch in range(1, num_epochs + 1):\n","        # Shuffle the data and target_encoded\n","        indices = np.arange(len(data))\n","        np.random.shuffle(indices)\n","        data_shuffled = data[indices]\n","        target_shuffled = target_encoded[indices]\n","        \n","        total_loss = 0\n","        correct_predictions = 0\n","        \n","        for start_idx in range(0, len(data), minibatch_size):\n","            end_idx = start_idx + minibatch_size\n","            inputs = data_shuffled[start_idx:end_idx]\n","            targets = target_shuffled[start_idx:end_idx]\n","            \n","            # Forward pass\n","            predictions = mlp.forward(inputs)\n","\n","            # Calculate loss\n","            cce_loss = CategoricalCrossEntropyLoss()\n","            loss = cce_loss(predictions, targets)\n","            total_loss += loss\n","            \n","            # Calculate accuracy\n","            correct_predictions += np.sum(np.argmax(predictions, axis=1) == np.argmax(targets, axis=1))\n","            \n","            # Backward pass and weight updates\n","            mlp.backward(predictions, targets)\n","        \n","        # Calculate average loss and accuracy for the epoch\n","        avg_loss = total_loss / len(data)\n","        accuracy = correct_predictions / len(data)\n","        \n","        loss_history.append(avg_loss)\n","        accuracy_history.append(accuracy)\n","        \n","        print(f\"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy * 100:.2f}%\")\n","    \n","    # Plot the loss and accuracy\n","    plt.figure(figsize=(12, 4))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(range(1, num_epochs + 1), loss_history)\n","    plt.title('Average Loss vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Average Loss')\n","    \n","    plt.subplot(1, 2, 2)\n","    plt.plot(range(1, num_epochs + 1), accuracy_history)\n","    plt.title('Accuracy vs. Epoch')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    \n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Example usage:"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"CategoricalCrossEntropyLoss() takes no arguments","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\hawej\\Desktop\\Semester7\\ANN Winter_23_24\\ANN_HKremer\\-IANNwTf-Group17\\Homework1_1.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mlp \u001b[39m=\u001b[39m MLP(layer_sizes\u001b[39m=\u001b[39m[\u001b[39m35\u001b[39m] , activation\u001b[39m=\u001b[39mSigmoidActivation(), input_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cce_loss \u001b[39m=\u001b[39m CategoricalCrossEntropyLoss()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_mlp(mlp, data, target_encoded, learning_rate, num_epochs, minibatch_size)\n","\u001b[1;32mc:\\Users\\hawej\\Desktop\\Semester7\\ANN Winter_23_24\\ANN_HKremer\\-IANNwTf-Group17\\Homework1_1.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predictions \u001b[39m=\u001b[39m mlp\u001b[39m.\u001b[39mforward(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m CategoricalCrossEntropyLoss(predictions, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hawej/Desktop/Semester7/ANN%20Winter_23_24/ANN_HKremer/-IANNwTf-Group17/Homework1_1.ipynb#X25sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n","\u001b[1;31mTypeError\u001b[0m: CategoricalCrossEntropyLoss() takes no arguments"]}],"source":["num_epochs = 10\n","learning_rate = 0.01\n","minibatch_size = 20\n","mlp = MLP(layer_sizes=[35] , activation=SigmoidActivation(), input_size=64)\n","cce_loss = CategoricalCrossEntropyLoss()\n","train_mlp(mlp, data, target_encoded, learning_rate, num_epochs, minibatch_size)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":2}
