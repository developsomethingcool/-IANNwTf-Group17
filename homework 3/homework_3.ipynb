{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "xF4l5aY4MyT-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prep_cifar(batch_size, shuffle_buffer_size):\n",
        "    train, test = tfds.load('cifar10', split=['train', 'test'], as_supervised=True)\n",
        "\n",
        "    def preprocessing_func(img, label):\n",
        "        img = tf.cast(img, tf.float32)\n",
        "        img = (img / 128) - 1\n",
        "        label = tf.one_hot(label, depth=10)\n",
        "        return img, label\n",
        "\n",
        "    train = train.map(lambda img, label: preprocessing_func(img, label))\n",
        "    test = test.map(lambda img, label: preprocessing_func(img, label))\n",
        "\n",
        "    train = train.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(2)\n",
        "    test = test.batch(batch_size).prefetch(2)\n",
        "\n",
        "    return train, test\n"
      ],
      "metadata": {
        "id": "X8uJy5k4NApt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "LR = 0.001\n",
        "NUM_EPOCHS = 50"
      ],
      "metadata": {
        "id": "Y29_MaxPNEH2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "train_data, test_data = load_and_prep_cifar(batch_size=BATCH_SIZE, shuffle_buffer_size=SHUFFLE_BUFFER_SIZE)\n",
        "\n",
        "# # Visualize a sample from the training dataset\n",
        "# sample_batch = next(iter(train_data))\n",
        "# images, labels = sample_batch\n",
        "\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# for i in range(9):\n",
        "#     plt.subplot(3, 3, i + 1)\n",
        "#     plt.imshow(images[i].numpy() / 2 + 0.5)  # Rescale images to [0, 1]\n",
        "#     plt.title(f\"Label: {tf.argmax(labels[i]).numpy()}\")\n",
        "#     plt.axis(\"off\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "qy34NzFtNHYt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cifar_10(model, train_ds, test_ds, num_epochs, lr):\n",
        "    loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
        "\n",
        "    train_losses, train_accuracies = [], []\n",
        "    test_losses, test_accuracies = [], []\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(images, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(images)\n",
        "            loss = loss_object(labels, predictions)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(labels, predictions)\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(images, labels):\n",
        "        predictions = model(images)\n",
        "        t_loss = loss_object(labels, predictions)\n",
        "\n",
        "        test_loss(t_loss)\n",
        "        test_accuracy(labels, predictions)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training loop\n",
        "        for images, labels in train_ds:\n",
        "            train_step(images, labels)\n",
        "\n",
        "        # Testing loop\n",
        "        for test_images, test_labels in test_ds:\n",
        "            test_step(test_images, test_labels)\n",
        "\n",
        "        # Save metrics for plotting\n",
        "        train_losses.append(train_loss.result().numpy())\n",
        "        train_accuracies.append(train_accuracy.result().numpy())\n",
        "        test_losses.append(test_loss.result().numpy())\n",
        "        test_accuracies.append(test_accuracy.result().numpy())\n",
        "\n",
        "        template = 'Epoch {}, Loss: {:.4f}, Accuracy: {:.2f}%, Test Loss: {:.4f}, Test Accuracy: {:.2f}%'\n",
        "        print(template.format(epoch + 1,\n",
        "                              train_loss.result(),\n",
        "                              train_accuracy.result() * 100,\n",
        "                              test_loss.result(),\n",
        "                              test_accuracy.result() * 100))\n",
        "\n",
        "        # Reset the metrics for the next epoch\n",
        "        train_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "        test_loss.reset_states()\n",
        "        test_accuracy.reset_states()\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train')\n",
        "    plt.plot(test_losses, label='Test')\n",
        "    plt.title('Training and Testing Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accuracies, label='Train')\n",
        "    plt.plot(test_accuracies, label='Test')\n",
        "    plt.title('Training and Testing Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0ETSD7W8SFaP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile the model\n",
        "def create_my_cifar_cnn(name='my_cnn'):\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3), dtype=tf.float32)\n",
        "\n",
        "    cnn_layer_1_1= tf.keras.layers.Conv2D(filters = 16, kernel_size=(3,3), activation = 'relu',padding='same')\n",
        "    x= cnn_layer_1_1(inputs)#shape: [batch_size, 32,32,16]\n",
        "\n",
        "    cnn_layer_1_2= tf.keras.layers.Conv2D(filters = 16, kernel_size=(3,3), activation = 'relu',padding='same')\n",
        "    x= cnn_layer_1_2(x)#shape: [batch_size, 32,32,16]\n",
        "\n",
        "    cnn_layer_pooling1 = tf.keras.layers.MaxPool2D()\n",
        "    x= cnn_layer_pooling1(x)#shape: [batch_size, 16,16,16]\n",
        "\n",
        "    cnn_layer_2_1= tf.keras.layers.Conv2D(filters = 32, kernel_size=(3,3), activation = 'relu',padding='same')\n",
        "    x= cnn_layer_2_1(x)#shape: [batch_size, 16,16,32]\n",
        "\n",
        "    cnn_layer_2_2= tf.keras.layers.Conv2D(filters = 32, kernel_size=(3,3), activation = 'relu',padding='same')\n",
        "    x= cnn_layer_2_2(x)#shape: [batch_size, 16,16,32]\n",
        "\n",
        "    cnn_layer_pooling2 = tf.keras.layers.MaxPool2D()\n",
        "    x= cnn_layer_pooling2(x)#shape: [batch_size, 8,8,32]\n",
        "\n",
        "    cnn_layer_3_1= tf.keras.layers.Conv2D(filters = 64, kernel_size=(3,3), activation = 'relu',padding='same')\n",
        "    x= cnn_layer_3_1(x)#shape: [batch_size, 8,8,64]\n",
        "\n",
        "    cnn_layer_3_2= tf.keras.layers.Conv2D(filters = 64, kernel_size=(3,3), activation = 'relu',padding='same')\n",
        "    x= cnn_layer_3_2(x)#shape: [batch_size, 8,8,64]\n",
        "\n",
        "    cnn_layer_flatten = tf.keras.layers.Flatten()\n",
        "    x= cnn_layer_flatten(x)# shape: [batch_size, 4096]\n",
        "\n",
        "    dense_layer_1 = tf.keras.layers.Dense(64, activation = 'relu')\n",
        "    x= dense_layer_1(x)#shape: [batch_size, 64]\n",
        "\n",
        "    dense_layer_2 = tf.keras.layers.Dense(32, activation = 'relu')\n",
        "    x= dense_layer_2(x)#shape: [batch_size, 32]\n",
        "\n",
        "    output_layer = tf.keras.layers.Dense(10, activation = 'softmax')\n",
        "    output = output_layer(x)#shape: [batch_size, 10]\n",
        "\n",
        "    model = tf.keras.Model(inputs , output,name=name)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3Xzy0K8JNMCo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_my_cifar_cnn()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the custom training loop\n",
        "train_cifar_10(model, train_data, test_data, num_epochs=NUM_EPOCHS, lr=LR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI5xJTrbNaIe",
        "outputId": "4ffcc8e6-05f4-4904-dd56-bd1483d67b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.5423, Accuracy: 43.35%, Test Loss: 1.2651, Test Accuracy: 54.33%\n",
            "Epoch 2, Loss: 1.0943, Accuracy: 60.83%, Test Loss: 1.0594, Test Accuracy: 62.53%\n",
            "Epoch 3, Loss: 0.9035, Accuracy: 67.92%, Test Loss: 0.8775, Test Accuracy: 68.86%\n",
            "Epoch 4, Loss: 0.7775, Accuracy: 72.59%, Test Loss: 0.8223, Test Accuracy: 71.48%\n",
            "Epoch 5, Loss: 0.6734, Accuracy: 76.22%, Test Loss: 0.8192, Test Accuracy: 72.56%\n",
            "Epoch 6, Loss: 0.5935, Accuracy: 79.23%, Test Loss: 0.8205, Test Accuracy: 72.97%\n",
            "Epoch 7, Loss: 0.5139, Accuracy: 82.00%, Test Loss: 0.8761, Test Accuracy: 72.74%\n",
            "Epoch 8, Loss: 0.4451, Accuracy: 84.45%, Test Loss: 0.9162, Test Accuracy: 73.58%\n",
            "Epoch 9, Loss: 0.3808, Accuracy: 86.77%, Test Loss: 0.9701, Test Accuracy: 72.85%\n",
            "Epoch 10, Loss: 0.3248, Accuracy: 88.62%, Test Loss: 1.0316, Test Accuracy: 73.82%\n",
            "Epoch 11, Loss: 0.2876, Accuracy: 89.84%, Test Loss: 1.1229, Test Accuracy: 72.49%\n",
            "Epoch 12, Loss: 0.2494, Accuracy: 91.19%, Test Loss: 1.2474, Test Accuracy: 72.57%\n",
            "Epoch 13, Loss: 0.2183, Accuracy: 92.21%, Test Loss: 1.2950, Test Accuracy: 71.41%\n",
            "Epoch 14, Loss: 0.1949, Accuracy: 93.33%, Test Loss: 1.3736, Test Accuracy: 72.63%\n",
            "Epoch 15, Loss: 0.1823, Accuracy: 93.74%, Test Loss: 1.3757, Test Accuracy: 73.01%\n",
            "Epoch 16, Loss: 0.1640, Accuracy: 94.27%, Test Loss: 1.5416, Test Accuracy: 72.56%\n",
            "Epoch 17, Loss: 0.1526, Accuracy: 94.66%, Test Loss: 1.5097, Test Accuracy: 73.13%\n",
            "Epoch 18, Loss: 0.1433, Accuracy: 94.92%, Test Loss: 1.6095, Test Accuracy: 71.80%\n",
            "Epoch 19, Loss: 0.1377, Accuracy: 95.28%, Test Loss: 1.5296, Test Accuracy: 72.81%\n",
            "Epoch 20, Loss: 0.1289, Accuracy: 95.57%, Test Loss: 1.6096, Test Accuracy: 73.87%\n",
            "Epoch 21, Loss: 0.1244, Accuracy: 95.79%, Test Loss: 1.6598, Test Accuracy: 72.78%\n",
            "Epoch 22, Loss: 0.1182, Accuracy: 95.97%, Test Loss: 1.7235, Test Accuracy: 72.35%\n",
            "Epoch 23, Loss: 0.1091, Accuracy: 96.29%, Test Loss: 1.7986, Test Accuracy: 72.29%\n",
            "Epoch 24, Loss: 0.1077, Accuracy: 96.29%, Test Loss: 1.7708, Test Accuracy: 72.78%\n",
            "Epoch 25, Loss: 0.1122, Accuracy: 96.09%, Test Loss: 1.7218, Test Accuracy: 73.15%\n",
            "Epoch 26, Loss: 0.0964, Accuracy: 96.68%, Test Loss: 1.7404, Test Accuracy: 73.10%\n",
            "Epoch 27, Loss: 0.1082, Accuracy: 96.27%, Test Loss: 1.8415, Test Accuracy: 72.53%\n",
            "Epoch 28, Loss: 0.0911, Accuracy: 96.88%, Test Loss: 1.9027, Test Accuracy: 72.51%\n",
            "Epoch 29, Loss: 0.0940, Accuracy: 96.88%, Test Loss: 1.8546, Test Accuracy: 72.75%\n"
          ]
        }
      ]
    }
  ]
}