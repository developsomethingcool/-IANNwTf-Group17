{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u3RCl2lvLRg"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN13jnRuwTnD"
      },
      "outputs": [],
      "source": [
        "( train_ds , test_ds ), ds_info = tfds.load('mnist', split =['train','test'], as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batchsize = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Oz8jiEwuoY"
      },
      "outputs": [],
      "source": [
        "def prepare_mnist_data(ds):\n",
        "  # Transform the dataset by mapping each element (feature_dict) to a tuple containing the \"image\" and \"label\" components.\n",
        "  #ds = ds.map(lambda feature_dict: (feature_dict[\"image\"], feature_dict[\"label\"]))\n",
        "\n",
        "  # Reshape the images by mapping each element (image, label) to a tuple with the image reshaped to a 1D vector and the label.\n",
        "  ds = ds.map(lambda image, label: (tf.reshape(image, (-1,)), label))\n",
        "\n",
        "  # Scale and normalize the images to the range [-1, 1] by mapping each element (image, label) to a tuple.\n",
        "  # This is done by casting the image to float32 and dividing by 128, then subtracting 1.\n",
        "  ds = ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label))\n",
        "\n",
        "  # One-hot encode the labels by mapping each element (image, label) to a tuple where the label is one-hot encoded with depth 10.\n",
        "  ds = ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
        "\n",
        "  # Shuffle the dataset with a buffer size of 1024 and batch the shuffled data into batches of size 256.\n",
        "  ds = ds.shuffle(1024).batch(batchsize)\n",
        "\n",
        "  # Prefetch the data to improve performance by overlapping data preprocessing and model execution.\n",
        "  ds = ds.prefetch(4)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGLpoXICPYc8"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_ds.apply(prepare_mnist_data)\n",
        "test_dataset = test_ds.apply(prepare_mnist_data)\n",
        "\n",
        "batchsize = 500\n",
        "\n",
        "train_dataset1 = train_ds.apply(prepare_mnist_data)\n",
        "test_dataset1 = test_ds.apply(prepare_mnist_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuWVHzo7SjGZ"
      },
      "outputs": [],
      "source": [
        "#Define the MLP Model\n",
        "class MLP_Model(tf.keras.Model):\n",
        "  def __init__(self, layer_sizes, output_size=10):\n",
        "    super().__init__()\n",
        "    self.mlp_layers = []\n",
        "\n",
        "    for layer_size in layer_sizes:\n",
        "      new_layer = tf.keras.layers.Dense(units=layer_size, activation=\"sigmoid\")\n",
        "      self.mlp_layers.append(new_layer)\n",
        "    self.output_layer = tf.keras.layers.Dense(units=output_size, activation=\"softmax\")\n",
        "\n",
        "  def call(self, x):\n",
        "    for layer in self.mlp_layers:\n",
        "      x = layer(x)\n",
        "    y = self.output_layer(x)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtNPRsCCM_RZ"
      },
      "outputs": [],
      "source": [
        "def train_mnist_model(train_ds, model, loss_func, optimizer, num_epochs):\n",
        "  # Initialize lists to store training and test loss and accuracy\n",
        "  train_loss_list = []\n",
        "  train_accuracy_list = []\n",
        "  # Initialize the accuracy metrics\n",
        "  train_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    total_train_loss = 0.0\n",
        "    train_accuracy_metric.reset_states()\n",
        "\n",
        "    for inputs, labels in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(inputs)\n",
        "            train_loss = loss_func(labels, outputs)\n",
        "\n",
        "        gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        total_train_loss += train_loss.numpy()\n",
        "        train_accuracy_metric.update_state(tf.argmax(labels, axis=1), tf.argmax(outputs, axis=1))\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_ds)\n",
        "    train_accuracy = train_accuracy_metric.result().numpy()\n",
        "\n",
        "    # Print or log the values\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "\n",
        "    # Store values for plotting\n",
        "    train_loss_list.append(average_train_loss)\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "\n",
        "  return (train_loss_list, train_accuracy_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNteLocHP0Sy"
      },
      "outputs": [],
      "source": [
        "def test_mnist_model(test_ds, model, loss_func, optimizer, num_epochs):\n",
        "  test_loss_list = []\n",
        "  test_accuracy_list = []\n",
        "\n",
        "  # Initialize the accuracy metrics\n",
        "  test_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Testing\n",
        "    total_test_loss = 0.0\n",
        "    test_accuracy_metric.reset_states()\n",
        "\n",
        "    for test_inputs, test_labels in test_ds:\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = loss_func(test_labels, test_outputs)\n",
        "\n",
        "        total_test_loss += test_loss.numpy()\n",
        "        test_accuracy_metric.update_state(tf.argmax(test_labels, axis=1), tf.argmax(test_outputs, axis=1))\n",
        "\n",
        "    average_test_loss = total_test_loss / len(test_ds)\n",
        "    test_accuracy = test_accuracy_metric.result().numpy()\n",
        "\n",
        "    # Print or log the values\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    # Store values for plotting\n",
        "    test_loss_list.append(average_test_loss)\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "\n",
        "  return (test_loss_list, test_accuracy_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY1bbTJkUlb6"
      },
      "outputs": [],
      "source": [
        "def train_test_mnist_model(train_ds, test_ds, model, loss_func, optimizer, num_epochs):\n",
        "  # Initialize lists to store training and test loss and accuracy\n",
        "  train_loss_list = []\n",
        "  train_accuracy_list = []\n",
        "  test_loss_list = []\n",
        "  test_accuracy_list = []\n",
        "\n",
        "  # Initialize the accuracy metrics\n",
        "  train_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "  test_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = 10  # You can adjust this based on your needs\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    total_train_loss = 0.0\n",
        "    train_accuracy_metric.reset_states()\n",
        "\n",
        "    for inputs, labels in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(inputs)\n",
        "            train_loss = loss_func(labels, outputs)\n",
        "\n",
        "        gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        total_train_loss += train_loss.numpy()\n",
        "        train_accuracy_metric.update_state(tf.argmax(labels, axis=1), tf.argmax(outputs, axis=1))\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_ds)\n",
        "    train_accuracy = train_accuracy_metric.result().numpy()\n",
        "\n",
        "    # Testing\n",
        "    total_test_loss = 0.0\n",
        "    test_accuracy_metric.reset_states()\n",
        "\n",
        "    for test_inputs, test_labels in test_ds:\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = loss_func(test_labels, test_outputs)\n",
        "\n",
        "        total_test_loss += test_loss.numpy()\n",
        "        test_accuracy_metric.update_state(tf.argmax(test_labels, axis=1), tf.argmax(test_outputs, axis=1))\n",
        "\n",
        "    average_test_loss = total_test_loss / len(test_ds)\n",
        "    test_accuracy = test_accuracy_metric.result().numpy()\n",
        "\n",
        "    # Print or log the values\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    # Store values for plotting\n",
        "    train_loss_list.append(average_train_loss)\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    test_loss_list.append(average_test_loss)\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "\n",
        "  return (train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7bXYQJhZVcw",
        "outputId": "74e81757-f116-4fcb-c875-22316a24eb0f"
      },
      "outputs": [],
      "source": [
        "model = MLP_Model([256,256])\n",
        "model1 = MLP_Model([64,32])\n",
        "model2 = MLP_Model([25,25])\n",
        "model3 = MLP_Model([20])\n",
        "model4 = MLP_Model([1])\n",
        "\n",
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n",
        "ds = train_dataset\n",
        "test_ds = test_dataset\n",
        "num_epochs = 8\n",
        "\n",
        "#Diffrent models with diffrent layers\n",
        "print(\"Model 0, SDG\")\n",
        "(train_loss_list0_0, train_accuracy_list0_0, test_loss_list0_0, test_accuracy_list0_0) = train_test_mnist_model(train_dataset, test_dataset, model, cce, optimizer, num_epochs)\n",
        "print(\"Model 0, ADAM\")\n",
        "(train_loss_list0_1, train_accuracy_list0_1, test_loss_list0_1, test_accuracy_list0_1) = train_test_mnist_model(train_dataset, test_dataset, model, cce, optimizer, num_epochs)\n",
        "print(\"Model 0, ARMSProp\")\n",
        "(train_loss_list0_2, train_accuracy_list0_2, test_loss_list0_2, test_accuracy_list0_2) = train_test_mnist_model(train_dataset, test_dataset, model, cce, optimizer, num_epochs)\n",
        "print(\"Model 1, SDG\")\n",
        "(train_loss_list1_0, train_accuracy_list1_0, test_loss_list1_0, test_accuracy_list1_0) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer, num_epochs)\n",
        "print(\"Model 1, ADAM\")\n",
        "(train_loss_list1_1, train_accuracy_list1_1, test_loss_list1_1, test_accuracy_list1_1) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer, num_epochs)\n",
        "print(\"Model 1, ARMSProp\")\n",
        "(train_loss_list1_2, train_accuracy_list1_2, test_loss_list1_2, test_accuracy_list1_2) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer, num_epochs)\n",
        "print(\"Model 2, SDG\")\n",
        "(train_loss_list2_0, train_accuracy_list2_0, test_loss_list2_0, test_accuracy_list2_0) = train_test_mnist_model(train_dataset, test_dataset, model2, cce, optimizer, num_epochs)\n",
        "print(\"Model 2, ADAM\")\n",
        "(train_loss_list2_1, train_accuracy_list2_1, test_loss_list2_1, test_accuracy_list2_1) = train_test_mnist_model(train_dataset, test_dataset, model2, cce, optimizer, num_epochs)\n",
        "print(\"Model 2, ARMSProp\")\n",
        "(train_loss_list2_2, train_accuracy_list2_2, test_loss_list2_2, test_accuracy_list2_2) = train_test_mnist_model(train_dataset, test_dataset, model2, cce, optimizer, num_epochs)\n",
        "print(\"Model 3, SDG\")\n",
        "(train_loss_list3_0, train_accuracy_list3_0, test_loss_list3_0, test_accuracy_list3_0) = train_test_mnist_model(train_dataset, test_dataset, model3, cce, optimizer, num_epochs)\n",
        "print(\"Model 3, ADAM\")\n",
        "(train_loss_list3_1, train_accuracy_list3_1, test_loss_list3_1, test_accuracy_list3_1) = train_test_mnist_model(train_dataset, test_dataset, model3, cce, optimizer, num_epochs)\n",
        "print(\"Model 3, ARMSProp\")\n",
        "(train_loss_list3_2, train_accuracy_list3_2, test_loss_list3_2, test_accuracy_list3_2) = train_test_mnist_model(train_dataset, test_dataset, model3, cce, optimizer, num_epochs)\n",
        "print(\"Model 4, SDG\")\n",
        "(train_loss_list4_0, train_accuracy_list4_0, test_loss_list4_0, test_accuracy_list4_0) = train_test_mnist_model(train_dataset, test_dataset, model4, cce, optimizer, num_epochs)\n",
        "print(\"Model 4, ADAM\")\n",
        "(train_loss_list4_1, train_accuracy_list4_1, test_loss_list4_1, test_accuracy_list4_1) = train_test_mnist_model(train_dataset, test_dataset, model4, cce, optimizer, num_epochs)\n",
        "print(\"Model 4, ARMSProp\")\n",
        "(train_loss_list4_2, train_accuracy_list4_2, test_loss_list4_2, test_accuracy_list4_2) = train_test_mnist_model(train_dataset, test_dataset, model4, cce, optimizer, num_epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AwtDIAsLnhc"
      },
      "outputs": [],
      "source": [
        "def visualization(train_losses, train_accuracies, test_losses, test_accuracies, name):\n",
        "    \"\"\"Visualizes accuracy and loss for training and test data using\n",
        "    the mean of each epoch.\n",
        "    Loss is displayed in a regular line, accuracy in a dotted\n",
        "    line.\n",
        "    Training data is displayed in blue, test data in red.\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_losses : numpy.ndarray\n",
        "    training losses\n",
        "    train_accuracies : numpy.ndarray\n",
        "    training accuracies\n",
        "    test_losses : numpy.ndarray\n",
        "    test losses\n",
        "    test_accuracies : numpy.ndarray\n",
        "    test accuracies\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    line1, = plt.plot(train_losses, \"b-\")\n",
        "    line2, = plt.plot(test_losses, \"r-\")\n",
        "    line3, = plt.plot(train_accuracies, \"b:\")\n",
        "    line4, = plt.plot(test_accuracies, \"r:\")\n",
        "    plt.xlabel(\"Training steps\")\n",
        "    plt.ylabel(\"Loss / Accuracy\")\n",
        "    plt.title(name)\n",
        "    plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"train accuracy\", \"test accuracy\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "tUKiaIbuSpCP",
        "outputId": "4471a8a0-664d-4f29-b6b4-875ad88abdb5"
      },
      "outputs": [],
      "source": [
        "visualization(train_loss_list0_0, train_accuracy_list0_0, test_loss_list0_0, test_accuracy_list0_0, name=\"Model 0_0\")\n",
        "visualization(train_loss_list1_0, train_accuracy_list1_0, test_loss_list1_0, test_accuracy_list1_0, name=\"Model 1_0\")\n",
        "visualization(train_loss_list2_0, train_accuracy_list2_0, test_loss_list2_0, test_accuracy_list2_0, name=\"Model 2_0\")\n",
        "visualization(train_loss_list3_0, train_accuracy_list3_0, test_loss_list3_0, test_accuracy_list3_0, name=\"Model 3_0\")\n",
        "visualization(train_loss_list4_0, train_accuracy_list4_0, test_loss_list4_0, test_accuracy_list4_0, name=\"Model 4_0\")\n",
        "\n",
        "visualization(train_loss_list0_1, train_accuracy_list0_1, test_loss_list0_1, test_accuracy_list0_1, name=\"Model 0_1\")\n",
        "visualization(train_loss_list1_1, train_accuracy_list1_1, test_loss_list1_1, test_accuracy_list1_1, name=\"Model 1_1\")\n",
        "visualization(train_loss_list2_1, train_accuracy_list2_1, test_loss_list2_1, test_accuracy_list2_1, name=\"Model 2_1\")\n",
        "visualization(train_loss_list3_1, train_accuracy_list3_1, test_loss_list3_1, test_accuracy_list3_1, name=\"Model 3_1\")\n",
        "visualization(train_loss_list4_1, train_accuracy_list4_1, test_loss_list4_1, test_accuracy_list4_1, name=\"Model 4_1\")\n",
        "\n",
        "visualization(train_loss_list0_2, train_accuracy_list0_2, test_loss_list0_2, test_accuracy_list0_2, name=\"Model 0_2\")\n",
        "visualization(train_loss_list1_2, train_accuracy_list1_2, test_loss_list1_2, test_accuracy_list1_2, name=\"Model 1_2\")\n",
        "visualization(train_loss_list2_2, train_accuracy_list2_2, test_loss_list2_2, test_accuracy_list2_2, name=\"Model 2_2\")\n",
        "visualization(train_loss_list3_2, train_accuracy_list3_2, test_loss_list3_2, test_accuracy_list3_2, name=\"Model 3_2\")\n",
        "visualization(train_loss_list4_2, train_accuracy_list4_2, test_loss_list4_2, test_accuracy_list4_2, name=\"Model 4_2\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trLErRV9S-P1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
