{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u3RCl2lvLRg"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN13jnRuwTnD"
      },
      "outputs": [],
      "source": [
        "( train_ds , test_ds ), ds_info = tfds.load('mnist', split =['train','test'], as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batchsize = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Oz8jiEwuoY"
      },
      "outputs": [],
      "source": [
        "def prepare_mnist_data(ds):\n",
        "  # Transform the dataset by mapping each element (feature_dict) to a tuple containing the \"image\" and \"label\" components.\n",
        "  #ds = ds.map(lambda feature_dict: (feature_dict[\"image\"], feature_dict[\"label\"]))\n",
        "\n",
        "  # Reshape the images by mapping each element (image, label) to a tuple with the image reshaped to a 1D vector and the label.\n",
        "  ds = ds.map(lambda image, label: (tf.reshape(image, (-1,)), label))\n",
        "\n",
        "  # Scale and normalize the images to the range [-1, 1] by mapping each element (image, label) to a tuple.\n",
        "  # This is done by casting the image to float32 and dividing by 128, then subtracting 1.\n",
        "  ds = ds.map(lambda image, label: ((tf.cast(image, tf.float32)/128)-1, label))\n",
        "\n",
        "  # One-hot encode the labels by mapping each element (image, label) to a tuple where the label is one-hot encoded with depth 10.\n",
        "  ds = ds.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
        "\n",
        "  # Shuffle the dataset with a buffer size of 1024 and batch the shuffled data into batches of size 256.\n",
        "  ds = ds.shuffle(1024).batch(batchsize)\n",
        "\n",
        "  # Prefetch the data to improve performance by overlapping data preprocessing and model execution.\n",
        "  ds = ds.prefetch(4)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGLpoXICPYc8"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_ds.apply(prepare_mnist_data)\n",
        "test_dataset = test_ds.apply(prepare_mnist_data)\n",
        "\n",
        "batchsize = 500\n",
        "\n",
        "train_dataset1 = train_ds.apply(prepare_mnist_data)\n",
        "test_dataset1 = test_ds.apply(prepare_mnist_data)\n",
        "\n",
        "batchsize = 1000\n",
        "\n",
        "train_dataset2 = train_ds.apply(prepare_mnist_data)\n",
        "test_dataset2 = test_ds.apply(prepare_mnist_data)\n",
        "\n",
        "batchsize = 20\n",
        "\n",
        "train_dataset3 = train_ds.apply(prepare_mnist_data)\n",
        "test_dataset3 = test_ds.apply(prepare_mnist_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuWVHzo7SjGZ"
      },
      "outputs": [],
      "source": [
        "#Define the MLP Model\n",
        "class MLP_Model(tf.keras.Model):\n",
        "  def __init__(self, layer_sizes, output_size=10):\n",
        "    super().__init__()\n",
        "    self.mlp_layers = []\n",
        "\n",
        "    for layer_size in layer_sizes:\n",
        "      new_layer = tf.keras.layers.Dense(units=layer_size, activation=\"sigmoid\")\n",
        "      self.mlp_layers.append(new_layer)\n",
        "    self.output_layer = tf.keras.layers.Dense(units=output_size, activation=\"softmax\")\n",
        "\n",
        "  def call(self, x):\n",
        "    for layer in self.mlp_layers:\n",
        "      x = layer(x)\n",
        "    y = self.output_layer(x)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY1bbTJkUlb6"
      },
      "outputs": [],
      "source": [
        "def train_test_mnist_model(train_ds, test_ds, model, loss_func, optimizer, num_epochs):\n",
        "  # Initialize lists to store training and test loss and accuracy\n",
        "  train_loss_list = []\n",
        "  train_accuracy_list = []\n",
        "  test_loss_list = []\n",
        "  test_accuracy_list = []\n",
        "\n",
        "  # Initialize the accuracy metrics\n",
        "  train_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "  test_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    total_train_loss = 0.0\n",
        "    train_accuracy_metric.reset_states()\n",
        "\n",
        "    for inputs, labels in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            outputs = model(inputs)\n",
        "            train_loss = loss_func(labels, outputs)\n",
        "\n",
        "        gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        total_train_loss += train_loss.numpy()\n",
        "        train_accuracy_metric.update_state(tf.argmax(labels, axis=1), tf.argmax(outputs, axis=1))\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_ds)\n",
        "    train_accuracy = train_accuracy_metric.result().numpy()\n",
        "\n",
        "    # Testing\n",
        "    total_test_loss = 0.0\n",
        "    test_accuracy_metric.reset_states()\n",
        "\n",
        "    for test_inputs, test_labels in test_ds:\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = loss_func(test_labels, test_outputs)\n",
        "\n",
        "        total_test_loss += test_loss.numpy()\n",
        "        test_accuracy_metric.update_state(tf.argmax(test_labels, axis=1), tf.argmax(test_outputs, axis=1))\n",
        "\n",
        "    average_test_loss = total_test_loss / len(test_ds)\n",
        "    test_accuracy = test_accuracy_metric.result().numpy()\n",
        "\n",
        "    # Print or log the values\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "    # Store values for plotting\n",
        "    train_loss_list.append(average_train_loss)\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    test_loss_list.append(average_test_loss)\n",
        "    test_accuracy_list.append(test_accuracy)\n",
        "\n",
        "  return (train_loss_list, train_accuracy_list, test_loss_list, test_accuracy_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7bXYQJhZVcw",
        "outputId": "74e81757-f116-4fcb-c875-22316a24eb0f"
      },
      "outputs": [],
      "source": [
        "model = MLP_Model([256,256])\n",
        "model1 = MLP_Model([64,32])\n",
        "model2 = MLP_Model([25,25])\n",
        "model3 = MLP_Model([20])\n",
        "model4 = MLP_Model([1])\n",
        "\n",
        "cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer_SDF = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n",
        "\n",
        "ds = train_dataset\n",
        "test_ds = test_dataset\n",
        "num_epochs = 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AwtDIAsLnhc"
      },
      "outputs": [],
      "source": [
        "def visualization_og(train_losses, train_accuracies, test_losses, test_accuracies, name):\n",
        "    \"\"\"Visualizes accuracy and loss for training and test data using\n",
        "    the mean of each epoch.\n",
        "    Loss is displayed in a regular line, accuracy in a dotted\n",
        "    line.\n",
        "    Training data is displayed in blue, test data in red.\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_losses : numpy.ndarray\n",
        "    training losses\n",
        "    train_accuracies : numpy.ndarray\n",
        "    training accuracies\n",
        "    test_losses : numpy.ndarray\n",
        "    test losses\n",
        "    test_accuracies : numpy.ndarray\n",
        "    test accuracies\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    line1, = plt.plot(train_losses, \"b-\")\n",
        "    line2, = plt.plot(test_losses, \"r-\")\n",
        "    line3, = plt.plot(train_accuracies, \"b:\")\n",
        "    line4, = plt.plot(test_accuracies, \"r:\")\n",
        "    plt.xlabel(\"Training steps\")\n",
        "    plt.ylabel(\"Loss / Accuracy\")\n",
        "    plt.title(name)\n",
        "    plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"train accuracy\", \"test accuracy\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualization(ax, train_losses, train_accuracies, test_losses, test_accuracies, name):\n",
        "    \"\"\"Adds accuracy and loss subplots for training and test data using\n",
        "    the mean of each epoch to an existing subplot.\n",
        "    Loss is displayed in a regular line, accuracy in a dotted\n",
        "    line.\n",
        "    Training data is displayed in blue, test data in red.\n",
        "    Parameters\n",
        "    ----------\n",
        "    ax : matplotlib.axes._subplots.AxesSubplot\n",
        "        Existing subplot to which the new subplots will be added.\n",
        "    train_losses : numpy.ndarray\n",
        "        Training losses\n",
        "    train_accuracies : numpy.ndarray\n",
        "        Training accuracies\n",
        "    test_losses : numpy.ndarray\n",
        "        Test losses\n",
        "    test_accuracies : numpy.ndarray\n",
        "        Test accuracies\n",
        "    name : str\n",
        "        Title for the subplots\n",
        "    \"\"\"\n",
        "    # Plot training and test losses\n",
        "    ax.plot(train_losses, \"b-\", label=\"Training Loss\")\n",
        "    ax.plot(test_losses, \"r-\", label=\"Test Loss\")\n",
        "\n",
        "    # Plot training and test accuracies\n",
        "    ax.plot(train_accuracies, \"b:\", label=\"Training Accuracy\")\n",
        "    ax.plot(test_accuracies, \"r:\", label=\"Test Accuracy\")\n",
        "\n",
        "    # Set labels and title\n",
        "    ax.set_xlabel(\"Training steps\")\n",
        "    ax.set_ylabel(\"Loss / Accuracy\")\n",
        "    ax.set_title(name)\n",
        "\n",
        "    # Add legends\n",
        "    ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diffrent_Layers():\n",
        "    #Diffrent models with diffrent layers SDG\n",
        "    print(\"Model 0, SDG\")\n",
        "    (train_loss_list0_0, train_accuracy_list0_0, test_loss_list0_0, test_accuracy_list0_0) = train_test_mnist_model(train_dataset, test_dataset, model, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"Model 1, SDG\")\n",
        "    (train_loss_list0_1, train_accuracy_list0_1, test_loss_list0_1, test_accuracy_list0_1) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"Model 2, SDG\")\n",
        "    (train_loss_list0_2, train_accuracy_list0_2, test_loss_list0_2, test_accuracy_list0_2) = train_test_mnist_model(train_dataset, test_dataset, model2, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"Model 3, SDG\")\n",
        "    (train_loss_list0_3, train_accuracy_list0_3, test_loss_list0_3, test_accuracy_list0_3) = train_test_mnist_model(train_dataset, test_dataset, model3, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"Model 4, SDG\")\n",
        "    (train_loss_list0_4, train_accuracy_list0_4, test_loss_list0_4, test_accuracy_list0_4) = train_test_mnist_model(train_dataset, test_dataset, model4, cce, optimizer_SDF, num_epochs)\n",
        "\n",
        "    fig, ((existing_ax_1_0, existing_ax_1_1), (existing_ax_1_2, existing_ax_1_3)) = plt.subplots(2,2)\n",
        "    fig.suptitle(\"Diffrent models with diffrent layers SDG\")\n",
        "    fig.set_size_inches(50, 50)\n",
        "    visualization(existing_ax_1_0, train_loss_list0_0, train_accuracy_list0_0, test_loss_list0_0, test_accuracy_list0_0, name=\"Model 0\")\n",
        "    visualization(existing_ax_1_0, train_loss_list0_1, train_accuracy_list0_1, test_loss_list0_1, test_accuracy_list0_1, name=\"Model 1\")\n",
        "    visualization(existing_ax_1_1, train_loss_list0_2, train_accuracy_list0_2, test_loss_list0_2, test_accuracy_list0_2, name=\"Model 2\")\n",
        "    visualization(existing_ax_1_2, train_loss_list0_3, train_accuracy_list0_3, test_loss_list0_3, test_accuracy_list0_3, name=\"Model 3\")\n",
        "    visualization(existing_ax_1_3, train_loss_list0_4, train_accuracy_list0_4, test_loss_list0_4, test_accuracy_list0_4, name=\"Model 4\")\n",
        "\n",
        "    visualization_og(train_loss_list0_0, train_accuracy_list0_0, test_loss_list0_0, test_accuracy_list0_0, name=\"Model 0\")\n",
        "\n",
        "diffrent_Layers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diffrent_Optimisers():\n",
        "    optimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "    optimizer_rmsprop = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)\n",
        "    optimizer_adadelta = tf.keras.optimizers.legacy.Adadelta(learning_rate=0.1)\n",
        "\n",
        "    #Diffrent optimiseres with same layer\n",
        "    print(\"SDG_Model_1\")\n",
        "    (train_loss_list1_0, train_accuracy_list1_O, test_loss_list1_O, test_accuracy_list1_O) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"Adam_Model_1\")\n",
        "    (train_loss_list1_1, train_accuracy_list1_1, test_loss_list1_1, test_accuracy_list1_1) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_adam, num_epochs)\n",
        "    print(\"RMSprop_Model_1\")\n",
        "    (train_loss_list1_2, train_accuracy_list1_2, test_loss_list1_2, test_accuracy_list1_2) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_rmsprop, num_epochs)\n",
        "    print(\"Adadelta_Model_1\")\n",
        "    (train_loss_list1_3, train_accuracy_list1_3, test_loss_list1_3, test_accuracy_list1_3) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_adadelta, num_epochs)\n",
        "\n",
        "    fig2, ((existing_ax_2_0, existing_ax_2_1), (existing_ax_2_2, existing_ax_2_3)) = plt.subplots(2,2)\n",
        "    fig2.set_size_inches(50, 50)\n",
        "    fig2.suptitle(\"Diffrent optimisers with same layer\")\n",
        "    visualization(existing_ax_2_0, train_loss_list1_0, train_accuracy_list1_O, test_loss_list1_O, test_accuracy_list1_O, name=\"SDG_Model\")\n",
        "    visualization(existing_ax_2_1, train_loss_list1_1, train_accuracy_list1_1, test_loss_list1_1, test_accuracy_list1_1, name=\"Adam_Model\")\n",
        "    visualization(existing_ax_2_2, train_loss_list1_2, train_accuracy_list1_2, test_loss_list1_2, test_accuracy_list1_2, name=\"RMSprop_Model\")\n",
        "    visualization(existing_ax_2_3, train_loss_list1_3, train_accuracy_list1_3, test_loss_list1_3, test_accuracy_list1_3, name=\"Adadelta_Model\")\n",
        "\n",
        "#diffrent_Optimisers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diffrent_Learning_Rates():\n",
        "    #SDG with diffrent learning rate\n",
        "    print(\"SDG_Model_1_learning_rate_0.1\")\n",
        "    (train_loss_list2_0, train_accuracy_list2_0, test_loss_list2_0, test_accuracy_list2_0) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"SDG_Model_1_learning_rate_0.01\")\n",
        "    (train_loss_list2_1, train_accuracy_list2_1, test_loss_list2_1, test_accuracy_list2_1) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.01), num_epochs)\n",
        "    print(\"SDG_Model_1_learning_rate_0.001\")\n",
        "    (train_loss_list2_2, train_accuracy_list2_2, test_loss_list2_2, test_accuracy_list2_2) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.001), num_epochs)\n",
        "    print(\"SDG_Model_1_learning_rate_5\")\n",
        "    (train_loss_list2_2, train_accuracy_list2_2, test_loss_list2_2, test_accuracy_list2_2) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=5), num_epochs)\n",
        "\n",
        "    fig3, ((existing_ax_3_0, existing_ax_3_1), (existing_ax_3_2, existing_ax_3_3)) = plt.subplots(2,2)\n",
        "    fig3.suptitle(\"SDG with different learning rate\")\n",
        "    fig3.set_size_inches(50, 50)\n",
        "    plt.xlim(0, 5)\n",
        "\n",
        "    visualization(existing_ax_3_0, train_loss_list2_0, train_accuracy_list2_0, test_loss_list2_0, test_accuracy_list2_0, name=\"SDG_Model_1_learning_rate_0.1\")\n",
        "    visualization(existing_ax_3_1, train_loss_list2_1, train_accuracy_list2_1, test_loss_list2_1, test_accuracy_list2_1, name=\"SDG_Model_1_learning_rate_0.01\")\n",
        "    visualization(existing_ax_3_2, train_loss_list2_2, train_accuracy_list2_2, test_loss_list2_2, test_accuracy_list2_2, name=\"SDG_Model_1_learning_rate_0.001\")\n",
        "    visualization(existing_ax_3_3, train_loss_list2_2, train_accuracy_list2_2, test_loss_list2_2, test_accuracy_list2_2, name=\"SDG_Model_1_learning_rate_5\")\n",
        "\n",
        "#diffrent_Learning_Rates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diffrent_momentum():\n",
        "    #SDG with diffrent momentum\n",
        "    print(\"SDG_Model_1_momentum_0.1\")\n",
        "    (train_loss_list3_0, train_accuracy_list3_0, test_loss_list3_0, test_accuracy_list3_0) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.1, momentum=0.1), num_epochs)\n",
        "    print(\"SDG_Model_1_momentum_0.5\")\n",
        "    (train_loss_list3_1, train_accuracy_list3_1, test_loss_list3_1, test_accuracy_list3_1) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.1, momentum=0.5), num_epochs)\n",
        "    print(\"SDG_Model_1_momentum_0.9\")\n",
        "    (train_loss_list3_2, train_accuracy_list3_2, test_loss_list3_2, test_accuracy_list3_2) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.1, momentum=0.9), num_epochs)\n",
        "    print(\"SDG_Model_1_momentum_0.99\")\n",
        "    (train_loss_list3_3, train_accuracy_list3_3, test_loss_list3_3, test_accuracy_list3_3) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.1, momentum=0.99), num_epochs)\n",
        "\n",
        "    fig4, ((existing_ax_4_0, existing_ax_4_1), (existing_ax_4_2, existing_ax_4_3)) = plt.subplots(2,2)\n",
        "    fig4.suptitle(\"SDG with different momentum\")\n",
        "    fig4.set_size_inches(50, 50)\n",
        "    visualization(existing_ax_4_0, train_loss_list3_0, train_accuracy_list3_0, test_loss_list3_0, test_accuracy_list3_0, name=\"SDG_momentum_0.1\")\n",
        "    visualization(existing_ax_4_1, train_loss_list3_1, train_accuracy_list3_1, test_loss_list3_1, test_accuracy_list3_1, name=\"SDG_momentum_0.5\")\n",
        "    visualization(existing_ax_4_2, train_loss_list3_2, train_accuracy_list3_2, test_loss_list3_2, test_accuracy_list3_2, name=\"SDG_momentum_0.9\")\n",
        "    visualization(existing_ax_4_3, train_loss_list3_3, train_accuracy_list3_3, test_loss_list3_3, test_accuracy_list3_3, name=\"SDG_momentum_0.99\")\n",
        "\n",
        "#diffrent_momentum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diffrent_batch_size():\n",
        "    #SDG with diffrent batch size\n",
        "    print(\"SDG_Model_1_batch_size_128\")\n",
        "    (train_loss_list4_0, train_accuracy_list4_0, test_loss_list4_0, test_accuracy_list4_0) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"SDG_Model_1_batch_size_500\")\n",
        "    (train_loss_list4_1, train_accuracy_list4_1, test_loss_list4_1, test_accuracy_list4_1) = train_test_mnist_model(train_dataset1, test_dataset1, model1, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"SDG_Model_1_batch_size_1000\")\n",
        "    (train_loss_list4_2, train_accuracy_list4_2, test_loss_list4_2, test_accuracy_list4_2) = train_test_mnist_model(train_dataset2, test_dataset2, model1, cce, optimizer_SDF, num_epochs)\n",
        "    print(\"SDG_Model_1_batch_size_20\")\n",
        "    (train_loss_list4_3, train_accuracy_list4_3, test_loss_list4_3, test_accuracy_list4_3) = train_test_mnist_model(train_dataset3, test_dataset3, model1, cce, optimizer_SDF, num_epochs)\n",
        "\n",
        "    fig5, ((existing_ax_5_0, existing_ax_5_1), (existing_ax_5_2, existing_ax_5_3)) = plt.subplots(2,2)\n",
        "    fig5.suptitle(\"SDG with different batch size\")\n",
        "    fig5.set_size_inches(50, 50)\n",
        "    visualization(existing_ax_5_0, train_loss_list4_0, train_accuracy_list4_0, test_loss_list4_0, test_accuracy_list4_0, name=\"SDG_batch_size_128\")\n",
        "    visualization(existing_ax_5_1, train_loss_list4_1, train_accuracy_list4_1, test_loss_list4_1, test_accuracy_list4_1, name=\"SDG_batch_size_500\")\n",
        "    visualization(existing_ax_5_2, train_loss_list4_2, train_accuracy_list4_2, test_loss_list4_2, test_accuracy_list4_2, name=\"SDG_batch_size_1000\")\n",
        "    visualization(existing_ax_5_3, train_loss_list4_3, train_accuracy_list4_3, test_loss_list4_3, test_accuracy_list4_3, name=\"SDG_batch_size_20\")\n",
        "\n",
        "#diffrent_batch_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diffrent_combinations_ll_and_momentum():\n",
        "    \n",
        "    #Diffrent combinations\n",
        "    print(\"lower learning rate, higher momentum\")\n",
        "    (train_loss_list5_0, train_accuracy_list5_0, test_loss_list5_0, test_accuracy_list5_0) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.01, momentum=0.9), num_epochs)\n",
        "    print(\"higher learning rate, lower momentum\")\n",
        "    (train_loss_list5_1, train_accuracy_list5_1, test_loss_list5_1, test_accuracy_list5_1) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.5, momentum=0.1), num_epochs)\n",
        "    print(\"lower learning rate, lower momentum\")\n",
        "    (train_loss_list5_2, train_accuracy_list5_2, test_loss_list5_2, test_accuracy_list5_2) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.01, momentum=0.1), num_epochs)\n",
        "    print(\"higher learning rate, higher momentum\")\n",
        "    (train_loss_list5_3, train_accuracy_list5_3, test_loss_list5_3, test_accuracy_list5_3) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, tf.keras.optimizers.legacy.SGD(learning_rate=0.5, momentum=0.9), num_epochs)\n",
        "\n",
        "    fig6, ((existing_ax_6_0, existing_ax_6_1), (existing_ax_6_2, existing_ax_6_3)) = plt.subplots(2,2)\n",
        "    fig6.suptitle(\"Diffrent combinations\")\n",
        "    fig6.set_size_inches(50, 50)\n",
        "    visualization(existing_ax_6_0, train_loss_list5_0, train_accuracy_list5_0, test_loss_list5_0, test_accuracy_list5_0, name=\"lower learning rate, higher momentum\")\n",
        "    visualization(existing_ax_6_1, train_loss_list5_1, train_accuracy_list5_1, test_loss_list5_1, test_accuracy_list5_1, name=\"higher learning rate, lower momentum\")\n",
        "    visualization(existing_ax_6_2, train_loss_list5_2, train_accuracy_list5_2, test_loss_list5_2, test_accuracy_list5_2, name=\"lower learning rate, lower momentum\")\n",
        "    visualization(existing_ax_6_3, train_loss_list5_3, train_accuracy_list5_3, test_loss_list5_3, test_accuracy_list5_3, name=\"higher learning rate, higher momentum\")\n",
        "\n",
        "#diffrent_combinations_ll_and_momentum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def diffrent_epochs():\n",
        "    print(\"SDG_Model_1_epochs_10\")\n",
        "    (train_loss_list6_0, train_accuracy_list6_0, test_loss_list6_0, test_accuracy_list6_0) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, 10)\n",
        "    print(\"SDG_Model_1_epochs_20\")\n",
        "    (train_loss_list6_1, train_accuracy_list6_1, test_loss_list6_1, test_accuracy_list6_1) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, 20)\n",
        "    print(\"SDG_Model_1_epochs_50\")\n",
        "    (train_loss_list6_2, train_accuracy_list6_2, test_loss_list6_2, test_accuracy_list6_2) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, 50)\n",
        "    print(\"SDG_Model_1_epochs_100\")\n",
        "    (train_loss_list6_3, train_accuracy_list6_3, test_loss_list6_3, test_accuracy_list6_3) = train_test_mnist_model(train_dataset, test_dataset, model1, cce, optimizer_SDF, 100)\n",
        "\n",
        "    fig7, ((existing_ax_7_0, existing_ax_7_1), (existing_ax_7_2, existing_ax_7_3)) = plt.subplots(2,2)\n",
        "    fig7.suptitle(\"SDG with different epochs\")\n",
        "    fig7.set_size_inches(50, 50)\n",
        "    visualization(existing_ax_7_0, train_loss_list6_0, train_accuracy_list6_0, test_loss_list6_0, test_accuracy_list6_0, name=\"SDG_Model_1_epochs_10\")\n",
        "    visualization(existing_ax_7_1, train_loss_list6_1, train_accuracy_list6_1, test_loss_list6_1, test_accuracy_list6_1, name=\"SDG_Model_1_epochs_20\")\n",
        "    visualization(existing_ax_7_2, train_loss_list6_2, train_accuracy_list6_2, test_loss_list6_2, test_accuracy_list6_2, name=\"SDG_Model_1_epochs_50\")\n",
        "    visualization(existing_ax_7_3, train_loss_list6_3, train_accuracy_list6_3, test_loss_list6_3, test_accuracy_list6_3, name=\"SDG_Model_1_epochs_100\")\n",
        "    \n",
        "#diffrent_epochs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "tUKiaIbuSpCP",
        "outputId": "4471a8a0-664d-4f29-b6b4-875ad88abdb5"
      },
      "outputs": [],
      "source": [
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
